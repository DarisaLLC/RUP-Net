# -*- coding: utf-8 -*-
"""RUP-Net.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18QK4ppxI8krTHAC61yrobTv4LffGCrZZ

# **Residual U-Net + Pixel De-convolution Network**

1.   Import library
2.   Mount Google Drive to load dataset. Att: /X, /Y to load correctly.
"""

#@title Import { vertical-output: true, form-width: "40%", display-mode: "both" }
#%tensorflow_version 2.x
import tensorflow as tf
print("tensorflow version: ",tf.__version__)
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)


import h5py
import numpy as np
import matplotlib.pyplot as plt
import os
import pandas as pd


import tensorflow.keras as keras

from sklearn.metrics import roc_auc_score

import sklearn.metrics as sklm
import time


tf.executing_eagerly()
##@title Mount Google Drive { form-width: "10%", display-mode: "both" }
#from google.colab import drive
#drive.mount('gdrive')
##!ls "gdrive/My Drive/Deep-Learning/Data"


##!pip3 install tensorflow_io
##
##import tensorflow_io as tfio
##print("tensorflow_io version: ",tfio.__version__)

#@title Hyper-parameters { form-width: "10%", display-mode: "both" }
#number of training cases used in one step
BATCH_SIZE=2
#one epoch trains all training cases, One epoch=BATCH_SIZE*steps
EPOCH=1000

#3D image, train set (batch, in_depth, in_height, in_width, in_channels)
CHANNEL_AXIS=4

#number of filters at start node
START_NUM_OF_FILTERS=2

#number of class, for classification = 2
NUM_CLASS=2

#number of epoch to augment, if 2, 4*orig size new images are generated
#because one epoch including flip and rotate
AUGMENT_EPOCH=0

#Use residual network
ISRES=False
#Use MaxPooling or not
ISPOOL=True
#Deconvolution layer: deconv, pdn, upsampling
DECONV="deconv"
#loss function name: focal, dice, sce=default=softmax cross entropy
LOSS="focal"

#use Add or Concat in up-sampling
ISADD=False

#U-Net_DEPTH=first layer to bottom layer in U-Net, so there are layer_depth-1 bridges
UNET_DEPTH=2

#use Sparse or Cater CE
ISSPARSE=False


###############################
#how freq to save data to tensorboard, per SAVE_FREQ epoch
SAVE_FREQ=1
CKPT_FILE="./results/BestCKPT.ckpt"
CSV_FILE="./results/train.csv"
INIT_LEARNING_RATE=1e-1
##############################

root_dir="/home/hyao/Research/RUP-Net/Data/"
train_fn=root_dir+"train.h5"
test_fn=root_dir+"test.h5"

#!ls "$train_fn"

#x_train, y_train= tfio.IODataset.from_hdf5(train_fn, dataset='/X'), tfio.IODataset.from_hdf5(train_fn, dataset='/Y')
#x_test, y_test= tfio.IODataset.from_hdf5(test_fn, dataset='/X'), tfio.IODataset.from_hdf5(test_fn, dataset='/Y')

#conf=tf.io.read_file(root_dir+"Configures/train.csv")

#@title Reading file { form-width: "40%", display-mode: "both" }
#last channel axis is also number of class (1+1=2)
#image_shape=x_train.element_spec.shape
#print(image_shape)

with h5py.File(train_fn,'r') as train_f:
    x_train, y_train=np.array(train_f['X']), np.array(train_f['Y'])
    x_train=x_train/np.amax(x_train)
    y_train=y_train/np.amax(y_train)

SHUFFLE_SIZE=BATCH_SIZE*np.prod(y_train.shape)

if ISSPARSE:
    y_train=y_train[..., tf.newaxis]
else:
    y_train=tf.one_hot(tf.convert_to_tensor(y_train, dtype=tf.int32), 2, axis=CHANNEL_AXIS, dtype=tf.int32)
train_set=tf.data.Dataset.from_tensor_slices((x_train, y_train))


with h5py.File(test_fn,'r') as test_f:
    x_test, y_test=np.array(test_f['X']), np.array(test_f['Y'])
    x_test=x_test/np.amax(x_test)
    y_test=y_test/np.amax(y_test)

if ISSPARSE:
    y_test=y_test[..., tf.newaxis]
else:
    y_test = tf.one_hot(tf.convert_to_tensor(y_test, dtype=tf.int32), 2, axis=CHANNEL_AXIS, dtype=tf.int32)
#print(x_train.shape, y_train.shape)
#print(x_test.shape, y_test.shape)

#print(train_set.element_spec)
#print(np.amax(x_train))
#print(np.amax(y_train))
#print(np.amax(x_test))
#print(np.amax(y_test))
#



"""Augmentation"""

#@title Augment { vertical-output: true, form-width: "40%", display-mode: "both" }
#https://www.wouterbulten.nl/blog/tech/data-augmentation-using-tensorflow-data-dataset/#rotation-and-flipping
def plot_dataset(dataset, n_cases=8, n_slices_per_case=10):
    num_of_sets=len(dataset.element_spec)
    img_shape=dataset.element_spec[0].shape
    #[0]=slice/depth, [1]=width, [2]=height, [3]=color
    d,w,h,c=0,1,2,3
    dep,wid,hei=img_shape[d], img_shape[w], img_shape[h]
    slices=np.rint(np.linspace(0,1,n_slices_per_case)*(dep-1)).astype(np.int32)
    output = np.zeros((num_of_sets, hei * n_slices_per_case,wid * n_cases))

    i=0
    for case in dataset.take(n_cases):
        for j in range(num_of_sets):
            input=case[j].numpy()
            input=input[slices,:,:,0]
            output[j, :,i*wid:(i+1)*wid]=np.vstack(input)
        i += 1

    fig, ax = plt.subplots(1,num_of_sets, figsize=(15,15))
    for i in range(num_of_sets):
        #plt.imshow(output, extent=[1,n_cases, slices[-1], slices[0]])
        img=ax[i].imshow(output[i,:,:], cmap="gray", aspect="auto")
        xtick=np.arange(1,n_cases+1)
        ytick=slices/(slices[1]-slices[0])
        ax[i].set_xticks(xtick*wid-wid*0.5)
        ax[i].set_xticklabels(xtick)
        ax[i].set_yticks(ytick*hei+hei*0.5)
        ax[i].set_yticklabels(slices)
        #ax[i].set_title("Title")
        ax[i].set_xlabel("Case #")
        ax[i].set_ylabel("Slice #")
        fig.show(img)

#@title
def aug_rotate(img):
    nb=img.shape[0]
    outimg=[]
    for i in range(nb):
        outimg.append(tf.image.rot90(img[i,:,:,:], tf.random.uniform([], minval=1, maxval=4, dtype=tf.int32)))
    outimg=tf.stack(outimg)
    return outimg

def aug_flip(img):
    nb=img.shape[0]
    outimg=[]
    for i in range(nb):
        outimg.append(tf.image.flip_left_right(img[i,:,:,:]))
    outimg=tf.stack(outimg)
    return outimg

def dataset_len(dataset):
    i=0
    for e in dataset:
        i +=1
    return i
#for i in train_set.take(1):
#  ximg=i[0][45,:,:,0]
#  yimg=i[1][45,:,:,0]


if AUGMENT_EPOCH>0:
    augmentations = [aug_flip, aug_rotate]

    print("Before augmentation, train_set size = ",dataset_len(train_set))
    orig=train_set
    for i in range(AUGMENT_EPOCH):
        for f in augmentations:
            train_set=train_set.concatenate(orig.map(lambda x,y: (f(x), f(y)), num_parallel_calls=tf.data.experimental.AUTOTUNE))
    print("After augmentation, train_set size = ",dataset_len(train_set))
#for i in xxx.take(1):
#  ximg_aug=i[0][45,:,:,0]
#  yimg_aug=i[1][45,:,:,0]

#fig, ax = plt.subplots(2, 2)
#ax[0,0].imshow(ximg,cmap="gray")
#ax[0,1].imshow(ximg_aug,cmap="gray")
#ax[1,0].imshow(yimg, cmap="gray")
#ax[1,1].imshow(yimg_aug, cmap="gray")
#
#@title Set Train Set { vertical-output: true, form-width: "40%", display-mode: "both" }
#train_set=train_set.batch(BATCH_SIZE).shuffle(SHUFFLE_SIZE).prefetch(tf.data.experimental.AUTOTUNE)
train_set=train_set.batch(BATCH_SIZE).shuffle(SHUFFLE_SIZE)
print(train_set.element_spec)


def cal_focal(y_true, y_pred, alpha=0.25, gamma=2, is_to_mask=True):
    if is_to_mask:
        y_true_m=tomask(y_true)
        y_pred_m=tomask(y_pred)
    else:
        y_true_m = y_true
        y_pred_m = y_pred
    y_true = tf.cast(y_true_m, tf.float32)
    y_pred = tf.cast(y_pred_m, tf.float32)
    sigmoid_p = tf.nn.sigmoid(y_pred)
    zeros = tf.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)

    # For poitive prediction, only need consider front part loss, back part is 0;
    # y_true > zeros <=> z=1, so poitive coefficient = z - p.
    pos_p_sub = tf.where(y_true > zeros, y_true - sigmoid_p, zeros)

    # For negative prediction, only need consider back part loss, front part is 0;
    # y_true > zeros <=> z=1, so negative coefficient = 0.
    neg_p_sub = tf.where(y_true > zeros, zeros, sigmoid_p)
    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.math.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \
                          - (1 - alpha) * (neg_p_sub ** gamma) * tf.math.log(
        tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))
    return tf.reduce_sum(per_entry_cross_ent)


#@title Loss { form-width: "10%", display-mode: "both" }
class FocalLoss(keras.losses.Loss):
    def __init__(self, alpha=0.25, gamma=2,
                 reduction=keras.losses.Reduction.AUTO,
                 name='focal_loss'):
        super().__init__(reduction=reduction, name=name)
        self.alpha = alpha
        self.gamma = gamma

    def call(self, y_true, y_pred):
        #with tf.name_scope("focal_loss"):
        #with tf.compat.v1.get_default_graph().as_default(), tf.name_scope("focal_loss"):  # it seems doens't work
        #y_true=tomask((y_true))
        #y_pred=tomask((y_pred))
        return cal_focal(y_true, y_pred, alpha=self.alpha, gamma=self.gamma, is_to_mask=False)

#@title Metrics { vertical-output: true, form-width: "40%", display-mode: "both" }
def flatten(x, dtype=None):
    x = tf.reshape(x, [-1])
    if dtype is not None:
        x = tf.cast(x, dtype)
    return x

def softargmax(x, beta=1e10):
    x=tf.cast(x, dtype=tf.float32)
    x_range = tf.range(x.shape[-1], dtype=tf.float32)
    res=tf.reduce_sum(tf.nn.softmax(x*beta)*x_range,axis=-1)
    res=tf.cast(res, dtype=tf.int32)
    return res

def tomask(input, issoft=False):
    if issoft:
        #DEBUG: not working
        mask = softargmax(input)
    else:
        #mask = tf.argmax(input, axis=CHANNEL_AXIS, output_type=tf.int32)
        mask = tf.math.argmax(input, axis=CHANNEL_AXIS, output_type=tf.int32)
    mask = tf.expand_dims(mask, CHANNEL_AXIS)
    return mask

def cal_dice(y_true, y_pred, loss_type='jaccard', eps=1e-5, is_to_mask=True):
    #(Dice) Dice Coefficient
    if ISSPARSE:
        y_pred = tf.argmax(y_pred, axis=CHANNEL_AXIS, output_type=tf.int32)
        y_pred = y_pred[..., tf.newaxis]
    if is_to_mask:
        y_true_m=tomask(y_true)
        y_pred_m=tomask(y_pred)
    else:
        y_true_m = y_true
        y_pred_m = y_pred
    #y_true=tf.convert_to_tensor(y_true)
    y_true_f = flatten(y_true_m, tf.float32)
    #y_pred=tf.convert_to_tensor(y_pred)
    y_pred_f = flatten(y_pred_m, tf.float32)
    intersection = tf.reduce_sum(y_pred_f * y_true_f)
    if loss_type == 'jaccard':
        union = tf.reduce_sum(tf.square(y_pred_f)) + tf.reduce_sum(tf.square(y_true_f))
    elif loss_type == 'sorensen':
        union = tf.reduce_sum(y_pred_f) + tf.reduce_sum(y_true_f)
    else:
        raise ValueError("Unknown `loss_type`: %s" % loss_type)
    dice = (2 * intersection + eps) / (union + eps)
    return dice

def cal_accuracy(y_true, y_pred):
    y_true_m= tomask(y_true)
    y_pred_m= tomask(y_pred)
    correct_prediction = tf.equal(y_true_m, y_pred_m)
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    return accuracy

def cal_voe(y_true, y_pred, eps=1e-5):
    #(VOE)  Volumetric Overlap Error
    y_true_f = flatten(tomask(y_true), tf.float32)
    y_pred_f = flatten(tomask(y_pred), tf.float32)
    intersection = tf.reduce_sum(y_pred_f * y_true_f)
    denominator = tf.reduce_sum(tf.clip_by_value(y_true_f + y_pred_f, 0, 1))
    voe = (1 - intersection / (denominator + eps))
    return tf.reduce_mean(voe)

def cal_rvd(y_true, y_pred, eps=1e-5):
    #(RVD)   Relative Volume Difference
    y_true_f = flatten(tomask(y_true), tf.float32)
    y_pred_f = flatten(tomask(y_pred), tf.float32)
    y_true_sum = tf.reduce_sum(y_true_f)
    y_pred_sum = tf.reduce_sum(y_pred_f)
    rvd = tf.abs(y_pred_sum-y_true_sum)/(y_true_sum+eps)
    return tf.reduce_mean(rvd)


class MyAccuracy(keras.metrics.Metric):
    def __init__(self, name='Accuracy', **kwargs):
        super(MyAccuracy, self).__init__(name=name, **kwargs)
        self.accuracy = self.add_weight(name=name, initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        self.accuracy.assign(cal_accuracy(y_true, y_pred))

    def result(self):
        return self.accuracy

    def reset_states(self):
        # The state of the metric will be reset at the start of each epoch.
        self.accuracy.assign(0.)

class Dice(keras.metrics.Metric):
    def __init__(self, loss_type='jaccard', eps=1e-5, name='dice', **kwargs):
        super(Dice, self).__init__(name=name, **kwargs)
        self.dice = self.add_weight(name='dice', initializer='zeros')
        self.loss_type=loss_type
        self.eps=eps

    def update_state(self, y_true, y_pred, sample_weight=None):
        self.dice.assign(cal_dice(y_true, y_pred, self.loss_type, self.eps))

    def result(self):
        return self.dice

    def reset_states(self):
        # The state of the metric will be reset at the start of each epoch.
        self.dice.assign(0.)

class DiceLoss(keras.losses.Loss):
    def __init__(self, loss_type='jaccard', eps=1e-5,
                 reduction=keras.losses.Reduction.AUTO,
                 name='dice_loss'):
        super().__init__(reduction=reduction, name=name)
        self.loss_type=loss_type
        self.eps=eps

    def call(self, y_true, y_pred):
        dice=cal_dice(y_true, y_pred, self.loss_type, self.eps, is_to_mask=False)
        return (1-dice)

class SoftCrossEntropyLoss(keras.losses.Loss):
    def __init__(self, reduction=keras.losses.Reduction.AUTO, name='SCE_loss'):
        super().__init__(reduction=reduction, name=name)

    def call(self, y_true, y_pred):
        #with tf.name_scope(self.name):
            #y_true = tomask(y_true)
            #y_pred = tomask(y_pred)
        sce=tf.nn.softmax_cross_entropy_with_logits(y_true, y_pred)
        return sce


class AUC(keras.metrics.Metric):
    def __init__(self, name='auc', **kwargs):
        super(AUC, self).__init__(name=name, **kwargs)
        self.auc = self.add_weight(name='auc', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true=tf.reshape(y_true, [-1])
        y_pred=tf.reshape(y_pred, [-1])
        auc=tf.py_function(func=roc_auc_score, inp=[y_true, y_pred], Tout=tf.float32, name="sklearn/auc")
        self.auc.assign(auc)

    def result(self):
        return self.auc

    def reset_states(self):
        # The state of the metric will be reset at the start of each epoch.
        self.auc.assign(0.)

#@title Network Class/Def { form-width: "10%" }
def comb_layer(layers, isadd=True, scope=""):
    if isadd:
        addname="/Add"
        x = keras.layers.Add(name=scope+addname)(layers)
    else:
        addname="/Concat"
        x = keras.layers.Concatenate(name=scope+addname)(layers)
    return x, scope+addname

def node_new(input_layer, filters, conv_size, name=None, isres=True, isadd=True, filter_mul=1):
    #filter_mul: multiple filter by filter_mul**i
    #with tf.name_scope(name): #it seems doens't work
    x=input_layer
    num_sublayers=2
    for i in range(num_sublayers):
        str_i=str(i)
        #x = keras.layers.Conv3D((num_sublayers-i)*filters, conv_size, activation='relu', padding='same',
        #                        kernel_regularizer=keras.regularizers.l2(0.01), bias_regularizer=keras.regularizers.l2(0.01),
        #                        name=name+"/Conv3D_"+str_i)(x)
        x = keras.layers.Conv3D(int(filters*(filter_mul**i)), conv_size, activation=None, padding='same',
                                kernel_initializer='he_normal',
                                bias_initializer='he_uniform',
                                #kernel_regularizer=keras.regularizers.l2(0.01), bias_regularizer=keras.regularizers.l2(0.01),
                                name=name+"/Conv3D_"+str_i)(x)
        #x = keras.layers.BatchNormalization(name=name+"/BN_"+str_i)(x)
        #x = keras.layers.Activation("relu",name=name+"/Activation_"+str_i)(x)
        #x = keras.layers.Dropout(0.5, name=name+"/Dropout_"+str_i)(x)

    if isres:
        y = input_layer
        if isadd:
            y_channel=y.shape[-1]
            x_channel = x.shape[-1]
            mul=y_channel/x_channel
            if mul<1:
                mul=1/mul
            mul=int(mul)
            if mul != 1:
                y=tf.tile(input=y, multiples=tf.constant([1,1,1,1,mul]))
        x, cname=comb_layer(layers=[x, y], isadd=isadd, scope=name)
        #x = keras.layers.BatchNormalization(name=cname+"/Comb_BN")(x)

    return x

def u_net_new(input_layer, filters, conv_size, name=None, isres=True, ispool=False,
          layer_depth=4, deconv="deconv", isadd=True):
    x=input_layer
    downlist=[] #including input/output of each node in down-sampling operation

    #down-sampling
    for i in range(layer_depth-1):
        node_name="/Down_"+str(i)
        x=node_new(x, filters*(2**i), conv_size, name+node_name, isres, isadd, filter_mul=1)
        downlist.append(x)
        if ispool:
           x = keras.layers.MaxPooling3D(padding="same", name=name+node_name+"/Pool")(x)

    #bottom
    node_name="/Bottom"
    #x=node_new(x, filters*(2**(layer_depth-1)), conv_size, name+node_name, isres, isadd)
    x=node_new(x, filters*(2**(layer_depth-2)), conv_size, name+node_name, isres, isadd)

    def decoder(x, filters, conv_size, strides, name, deconv="deconv"):
        decname="/DeConv_Conv3DTrans"
        if deconv=="deconv":
            x=keras.layers.Conv3DTranspose(filters, conv_size, strides=strides, padding="same",
                                           kernel_initializer='he_normal',
                                           bias_initializer='he_uniform',
                                           #kernel_regularizer=keras.regularizers.l2(0.01),
                                           #bias_regularizer=keras.regularizers.l2(0.01),
                                           name=name+decname)(x)
        elif deconv=="upsampling":
            decname="/DeConv_UpSampling"
            #this 2 is same as one used for MaxPooling, so if pool changes, this will change
            x=keras.layers.UpSampling3D(strides, name=name+decname)(x)
        else:
            #implenment PDN here
            x=x
        #x = keras.layers.BatchNormalization(name=name+decname+"/BN")(x)
        #x = keras.layers.Activation('relu',name=name+decname+"/Activation")(x)
        return x

    #up-sampling
    strides=int(ispool)+1
    for i in range(layer_depth-2, -1, -1): #backward like 2,1,0 if dep=4
        node_name="/Up_"+str(i)
        x=decoder(x, filters*(2**i), conv_size, strides, name+node_name, deconv)
        x, cname=comb_layer(layers=[x, downlist[i]], isadd=isadd, scope=name+node_name+"/Bridge")
        #no pooling in up-sampling
        x=node_new(x, filters*(2**i), conv_size, name+node_name, isres, isadd=isadd, filter_mul=0.5)

    return x

#def node(input_layer, filters, conv_size, name=None, isres=True, ispool=False, isadd=True):
#    #with tf.compat.v1.get_default_graph().as_default(),tf.name_scope(name): #it seems doens't work
#    x=input_layer
#    num_sublayers=2
#    for i in range(num_sublayers):
#        str_i=str(i)
#        #x = keras.layers.Conv3D((num_sublayers-i)*filters, conv_size, activation='relu', padding='same',
#        #                        kernel_regularizer=keras.regularizers.l2(0.01), bias_regularizer=keras.regularizers.l2(0.01),
#        #                        name=name+"/Conv3D_"+str_i)(x)
#        x = keras.layers.Conv3D((num_sublayers-i)*filters, conv_size, activation=None, padding='same',
#                                #kernel_regularizer=keras.regularizers.l2(0.01), bias_regularizer=keras.regularizers.l2(0.01),
#                                name=name+"/Conv3D_"+str_i)(x)
#        x = keras.layers.BatchNormalization(name=name+"/BN_"+str_i)(x)
#        x = keras.layers.Dropout(0.5, name=name+"/Dropout_"+str_i)(x)
#
#    if isres:
#        x, cname=comb_layer(layers=[x, input_layer], isadd=isadd, scope=name)
#        x = keras.layers.BatchNormalization(name=cname+"/Comb_BN_0")(x)
#
#    if ispool:
#        x = keras.layers.MaxPooling3D(padding="same", name=name+"/Pool_0")(x)
#    #x = keras.layers.Activation('relu',name=name+"/Activation_0")(x)
#    return x
#
#def u_net(input_layer, filters, conv_size, name=None, isres=True, ispool=False,
#          layer_depth=4, deconv="deconv", isadd=True):
#    x=input_layer
#    downlist=[] #including input/output of each node in down-sampling operation
#
#    #down-sampling
#    for i in range(layer_depth-1):
#        downlist.append(x)
#        node_name="/Down_"+str(i)
#        x=node(x, filters, conv_size, name+node_name, isres, ispool, isadd)
#
#    downlist.append(x)
#
#    #bottom
#    node_name="/Bottom"
#    x=node(x, filters, conv_size, name+node_name, isres, ispool, isadd)
#
#
#    def decoder(x, filters, conv_size, strides, name, deconv="deconv"):
#        decname="/DeConv_Conv3DTrans_0"
#        if deconv=="deconv":
#            x=keras.layers.Conv3DTranspose(filters, conv_size, strides=strides, padding="same",
#                                           kernel_regularizer=keras.regularizers.l2(0.01),
#                                           bias_regularizer=keras.regularizers.l2(0.01),
#                                           name=name+decname)(x)
#        elif deconv=="upsampling":
#            decname="/DeConv_UpSampling_0"
#            #this 2 is same as one used for MaxPooling, so if pool changes, this will change
#            x=keras.layers.UpSampling3D(strides, name=name+decname)(x)
#        else:
#            #implenment PDN here
#            x=x
#        x=keras.layers.BatchNormalization(name=name+decname+"/BN_0")(x)
#        return x
#
#    #up-sampling
#    strides=int(ispool)+1
#    for i in range(layer_depth-2, -1, -1): #backward like 2,1,0 if dep=4
#        node_name="/Up_"+str(i)
#        x=decoder(x, filters, conv_size, strides, name+node_name, deconv)
#        #if isadd:
#        #  x=keras.layers.Add(name=name+node_name+"/Add_Down_"+str(i))([x, downlist[i+1]])
#        #else:
#        #  x=keras.layers.Concatenate(name=name+node_name+"/Concat_Down_"+str(i))([x, downlist[i+1]])
#        x, cname=comb_layer(layers=[x, downlist[i+1]], isadd=isadd, scope=name+node_name+"/"+str(i))
#        #no pooling in up-sampling
#        x=node(x, filters, conv_size, name+node_name, isres, ispool=False, isadd=isadd)
#
#    x=decoder(x, filters, conv_size, strides, name+node_name+"/Output", deconv)
#    #if isadd:
#    #  x=keras.layers.Add(name=name+"/Add_Input")([x, downlist[0]])
#    #else:
#    #  x=keras.layers.Concatenate(name=name+"/Concat_Input")([x, downlist[0]])
#    x, cname=comb_layer(layers=[x, downlist[0]], isadd=isadd, scope=name+"_Input")
#
#    return x

#@title Model { form-width: "10%", display-mode: "both" }
input_layer = keras.Input(shape=x_train.shape[1:], name="Input")
x=input_layer
#x = keras.layers.BatchNormalization(name="Input/BN")(x)

##use Conv3D to make (?, 64,64,64,1) to (?,64,64,64,32)
##x = keras.layers.Conv3D(START_NUM_OF_FILTERS, kernel_size=3, padding="same",activation='relu', name="Input/Conv3D")(x)
##use Concat to make (?, 64,64,64,1) to (?,64,64,64,32)
#for i in range(START_NUM_OF_FILTERS-1):
#  x=keras.layers.Concatenate(axis=CHANNEL_AXIS)([x, input_layer])
#x = u_net(input_layer=x, filters=START_NUM_OF_FILTERS, conv_size=3, name = "U-Net", isres=ISRES, ispool=ISPOOL,
#          layer_depth=UNET_DEPTH, deconv=DECONV, isadd=ISADD)


#y=x
#for i in range(2):
#  tmp = u_net(input_layer=y, filters=START_NUM_OF_FILTERS, conv_size=3+2*i, name = "U{}-Net".format(i), isres=ISRES, ispool=ISPOOL,
#            layer_depth=UNET_DEPTH, deconv=DECONV, isadd=ISADD)
#  if ISADD:
#    x=keras.layers.Add(name="U{}Add".format(i))([x,tmp])
#  else:
#    x=keras.layers.Concatenate(name="U{}Contac".format(i))([x,tmp])

for i in range(1):
    x = u_net_new(input_layer=x, filters=START_NUM_OF_FILTERS, conv_size=3, name = "U{}-Net".format(i), isres=ISRES, ispool=ISPOOL,
              layer_depth=UNET_DEPTH, deconv=DECONV, isadd=ISADD)

#for i in range(3):
#  x = u_net(input_layer=x, filters=START_NUM_OF_FILTERS, conv_size=3+i*2, name = "U{}-Net".format(i), isres=ISRES, ispool=ISPOOL,
#            layer_depth=UNET_DEPTH, deconv=DECONV, isadd=ISADD)
#  if ISADD:
#    x=keras.layers.Add(name="U{}Add".format(i))([x,input_layer])
#  else:
#    x=keras.layers.Concatenate(name="U{}Contac".format(i))([x,input_layer])

x = keras.layers.Conv3D(NUM_CLASS, kernel_size=3, padding="same", name="Output/Conv3D")(x)
#x = keras.layers.BatchNormalization(name="Output")(x)

model = keras.Model(input_layer, x, name="RUP-Net")

##@title Compile Model { vertical-output: true, form-width: "40%", display-mode: "both" }



if LOSS=="focal":
    loss=FocalLoss(alpha=0.25, gamma=2, name="focal_loss"),
elif LOSS=="dice":
    print("Using {} loss".format(LOSS))
    loss=DiceLoss(loss_type='jaccard', eps=1e-5, name='dice_loss')
else:
    #loss = SoftCrossEntropyLoss(name="SCE_loss")
    loss = keras.losses.CategoricalCrossentropy(from_logits=True)
    #if ISSPARSE:
    #    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    #else:
    #    loss = keras.losses.CategoricalCrossentropy(from_logits=True)

#model.compile(optimizer=keras.optimizers.Adam(),
#              loss=loss,
#              metrics=['accuracy', Dice(loss_type='jaccard', eps=1e-5, name='dice'), AUC(name='auc')])

model.compile(optimizer=keras.optimizers.Adam(learning_rate=INIT_LEARNING_RATE),
              loss=loss,
              #metrics=['accuracy'])
              metrics = [MyAccuracy(name="accuracy"), Dice(loss_type='jaccard', eps=1e-5, name='dice')])
#model.summary()
keras.utils.plot_model(model, 'RUP-Net.png', show_shapes=True)
#
##
###@title Train { form-width: "40%", display-mode: "both" }
###earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
###reduceLR=keras.callbacks.ReduceLROnPlateau(monitor="loss", factor=0.1, min_delta=1e-4, patience=3)
###csvlog = keras.callbacks.CSVLogger("train.csv",separator=',', append=False)
##

class TBCallback(keras.callbacks.TensorBoard):
    def __init__(self,
                 log_dir='logs',
                 histogram_freq=1,
                 write_graph=True,
                 write_images=False,
                 update_freq='epoch',
                 profile_batch=2,
                 embeddings_freq=0,
                 embeddings_metadata=None,
                 **kwargs):
        super(TBCallback, self).__init__(log_dir,
                                         histogram_freq,
                                         write_graph,
                                         write_images,
                                         update_freq,
                                         profile_batch,
                                         embeddings_freq,
                                         embeddings_metadata,
                                         **kwargs)

    def on_epoch_begin(self, epoch, logs=None):
        super(TBCallback, self).on_epoch_begin(epoch, logs)
        self.epoch_time_start=time.time()
        return

    def on_epoch_end(self, epoch, logs=None):
        super(TBCallback, self).on_epoch_end(epoch, logs)
        period=time.time()-self.epoch_time_start
        print("\n\nTraing time {:7.4f} s".format(period))
        print("logs = ",logs)
        for i in self._writers:
            with self._get_writer(i).as_default():
                if i == "train":
                    lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))
                    tf.summary.scalar('Learning Rate',lr,step=epoch)

        #save_layer_pos=[0.3,0.4,0.5,0.6]

        #for i in self._writers:
        #    if i == "train":
        #        x = x_train
        #        y = y_train
        #    else:
        #        x = x_test
        #        y = y_test
        #    #DEBUG check: 1 has images, 0 might not have.
        #    x=x[1][tf.newaxis,...]
        #    y=y[1][tf.newaxis,...]
        #    result = self.model.predict(x)
        #    with self._get_writer(i).as_default():
        #        nlayers=result.shape[1]
        #        #plt.imsave("result.jpg",result[0,19,:,:,0])
        #        tf.summary.scalar('Dice',cal_dice(y,result),step=epoch)
        #        tf.summary.scalar('VOE',cal_voe(y,result),step=epoch)
        #        tf.summary.scalar('RVD',cal_rvd(y,result),step=epoch)

        #        #zero_m=tf.zeros_like(y)
        #        #print("zero dice for cal_dice is ", cal_dice(y, zero_m, loss_type="sorensen",eps=0, is_to_mask=False))
        #        #print("self dice for cal_dice is ", cal_dice(y, y, loss_type="sorensen",eps=0, is_to_mask=False))
        #        #print("zero dice for cal_dice2 is ", cal_dice2(y, zero_m, loss_type="sorensen",eps=0, is_to_mask=False))
        #        #print("self dice for cal_dice2 is ", cal_dice2(y, y, loss_type="sorensen",eps=0, is_to_mask=False))
        #        #print("zero dice with mask for cal_dice is ", cal_dice(y, zero_m, loss_type="sorensen",eps=0, is_to_mask=True))
        #        #print("self dice with mask for cal_dice is ", cal_dice(y, y, loss_type="sorensen",eps=0, is_to_mask=True))
        #        #print("zero dice with mask for cal_dice2 is ", cal_dice2(y, zero_m, loss_type="sorensen",eps=0, is_to_mask=True))
        #        #print("self dice with mask for cal_dice2 is ", cal_dice2(y, y, loss_type="sorensen",eps=0, is_to_mask=True))

        #        maskdice=1-cal_dice(y, result, loss_type="jaccard",eps=0, is_to_mask=True)
        #        if maskdice>0.95:
        #            plt.imsave("y_true_mask.jpg", tomask(y)[0,19,:,:,0])
        #            plt.imsave("y_pred_mask.jpg", tomask(result)[0,19,:,:,0])
        #        print(i, "dice loss for cal_dice is ", 1-cal_dice(y, result, loss_type="jaccard",eps=0, is_to_mask=False))
        #        print(i, "dice loss with mask for cal_dice is ", 1-cal_dice(y, result, loss_type="jaccard",eps=0, is_to_mask=True))

        #        true_mask = tomask(y)
        #        pred_mask = tomask(result)
        #        print("{} true mask max {}".format(i, tf.reduce_max(true_mask)))
        #        print("{} pred mask max {}".format(i, tf.reduce_max(pred_mask)))
        #        for k in save_layer_pos:
        #            si=int(nlayers * k)
        #            tf.summary.image(i + "/Prediction", self.extract_layer_image(layer=pred_mask, batch_i=0, slice_i=si, feature_i=0), step=epoch)
        #            tf.summary.image(i + "/Truth", self.extract_layer_image(layer=true_mask, batch_i=0, slice_i=si, feature_i=0), step=epoch)




            #with self._get_writer(i).as_default():
            #    #for i in layer_outputs:
            #    #    print(i.name)
            #    #layers=[]
            #    #for i in ['Output/Conv3D', 'Output']:
            #    #    layers.append(self.model.get_layer(i+"/Identity:0").output)
            #    #print("y test max is ", tf.reduce_max(y_test))
            #    #pred_mask = tf.argmax(y_test, axis=CHANNEL_AXIS, output_type=tf.int32)
            #    #pred_mask = pred_mask[..., tf.newaxis]
            #    #print("y test pred mask shape is ", pred_mask.shape)
            #    #for k in [0.3,0.4,0.5,0.6]:

            #    for j,value in enumerate(result):
            #        nlayers=value.shape[1]
            #        if layer_outputs[j].name=="Output/Identity:0":
            #            pred_mask = tomask(value)
            #            for k in [0.3, 0.4, 0.5, 0.6]:
            #                #tf.compat.v2.summary.image(scope + "/" + layer_outputs[i].name,
            #                #                       self.extract_layer_image(layer=value, batch_i=0, slice_i=int(nlayers * k),
            #                #                                                feature_i=0), step=epoch)
            #                #tf.compat.v2.summary.image(scope + "/" + layer_outputs[i].name+"/1", self.extract_layer_image(layer=value, batch_i=0, slice_i=int(nlayers * k),
            #                #                                    feature_i=1), step=epoch)
            #                #res=np.asarray(res)
            #                #pred_mask=tf.argmax(value, axis=CHANNEL_AXIS, output_type=tf.int32)
            #                tf.summary.image(i + "/" + "Prediction",
            #                                 self.extract_layer_image(layer=pred_mask, batch_i=0, slice_i=int(nlayers * k),
            #                                                          feature_i=0), step=epoch)

            #            if i == "train":
            #                y=y_train
            #            else:
            #                y=y_test
            #            true_mask = tomask(y)
            #            tf.summary.scalar('Dice',tf.reduce_mean(cal_dice(true_mask,pred_mask)),step=epoch)
            #            tf.summary.scalar('VOE',tf.reduce_mean(cal_voe(true_mask,pred_mask)),step=epoch)
            #            tf.summary.scalar('RVD',tf.reduce_mean(cal_rvd(true_mask,pred_mask)),step=epoch)
            #            for k in [0.3,0.4,0.5,0.6]:
            #                tf.summary.image(i + "/" + "Truth", self.extract_layer_image(layer=true_mask, batch_i=1, slice_i=int(nlayers * k), feature_i=0), step=epoch)

    def mask_to_image(self, mask):
        my_cm = plt.cm.get_cmap('gray')
        m_max=np.max(mask)
        m_min=np.min(mask)
        if abs(m_max)<1e-5:
            norm_mask=mask
        else:
            norm_mask = (mask - m_min) / (m_max - m_min)
        img = my_cm(norm_mask)
        return img

    def extract_layer_image(self, layer, batch_i=0, slice_i=0, feature_i=0):
        '''
        extract one layer image from CNN 3d network

        :param layer: which network layer
        :param feature_i: which feature of the network layer
        :param img_layer: image layer
        :return: 2d image
        '''

        img = layer[batch_i, slice_i, :, :, feature_i]
        img = self.mask_to_image(img)
        img = tf.expand_dims(img, 0)
        img_name="batch{}_slice{}_feature{}".format(batch_i, slice_i, feature_i)
        return tf.reshape(img, img.shape,img_name)



#import tensorboard
base_log_dir="logs/"
log_dir = base_log_dir + time.strftime("%Y-%m-%d-%H-%M-%S", time.localtime())
#file_writer = tf.summary.create_file_writer(log_dir + "/train")
##file_writer.set_as_default()

outputFolder=os.path.dirname(CKPT_FILE)
if not os.path.exists(outputFolder):
    os.makedirs(outputFolder)

#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, write_images=True)
tb_callback = TBCallback(log_dir=log_dir, write_images=False, profile_batch=0, update_freq=SAVE_FREQ)
checkpointer = keras.callbacks.ModelCheckpoint(filepath=CKPT_FILE,
                                               monitor = 'val_dice',
                                               mode = 'max',
                                               verbose=1,
                                               save_best_only=True,
                                               save_weights_only=True
                                               )

from tensorflow.python.keras import backend as K

class DynamicLR(tf.keras.callbacks.Callback):
    """Reduce learning rate when a metric has stopped improving.
    Models often benefit from reducing the learning rate by a factor
    of 2-10 once learning stagnates. This callback monitors a
    quantity and if no improvement is seen for a 'patience' number
    of epochs, the learning rate is reduced.
    Example:
    ```python
    dynamic_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                                  patience=5, min_lr=0.001)
    model.fit(X_train, Y_train, callbacks=[dynamic_lr])
    ```
    Arguments:
        monitor: quantity to be monitored.
        factor: factor by which the learning rate will be reduced. new_lr = lr *
          factor
        patience: number of epochs with no improvement after which learning rate
          will be reduced.
        verbose: int. 0: quiet, 1: update messages.
        mode: one of {auto, min, max}. In `min` mode, lr will be reduced when the
          quantity monitored has stopped decreasing; in `max` mode it will be
          reduced when the quantity monitored has stopped increasing; in `auto`
          mode, the direction is automatically inferred from the name of the
          monitored quantity.
        min_delta: threshold for measuring the new optimum, to only focus on
          significant changes.
        cooldown: number of epochs to wait before resuming normal operation after
          lr has been reduced.
        min_lr: lower bound on the learning rate.
    """

    def __init__(self,
                 monitor='val_loss',
                 factor=0.1,
                 patience=10,
                 verbose=0,
                 mode='auto',
                 min_delta=1e-4,
                 cooldown=0,
                 min_lr=1e-5,
                 max_lr=100,
                 min_monitor=1e-2,
                 **kwargs):
        super(DynamicLR, self).__init__()

        self.monitor = monitor
        if 'epsilon' in kwargs:
            min_delta = kwargs.pop('epsilon')
        self.factor = factor
        self.min_lr = min_lr
        self.max_lr = max_lr
        self.min_monitor = min_monitor
        self.min_delta = min_delta
        self.patience = patience
        self.verbose = verbose
        self.cooldown = cooldown
        self.cooldown_counter = 0  # Cooldown counter.
        self.wait = 0
        self.best = 0
        self.mode = mode
        self.monitor_op = None
        self._reset()

    def _reset(self):
        """Resets wait counter and cooldown counter.
        """
        if self.mode not in ['auto', 'min', 'max']:
            self.mode = 'auto'
        if (self.mode == 'min' or
                (self.mode == 'auto' and 'acc' not in self.monitor)):
            self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)
            self.best = np.Inf
        else:
            self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)
            self.best = -np.Inf
        self.cooldown_counter = 0
        self.wait = 0

    def on_train_begin(self, logs=None):
        self._reset()

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        logs['lr'] = K.get_value(self.model.optimizer.lr)
        current = logs.get(self.monitor)
        if current is None:
            print(current)
        else:
            if self.in_cooldown():
                self.cooldown_counter -= 1
                self.wait = 0

            if self.monitor_op(current, self.best):
                self.best = current
                self.wait = 0
            elif not self.in_cooldown():
                self.wait += 1
                if self.wait >= self.patience:
                    old_lr = float(logs['lr'])
                    #if old_lr > self.min_lr:
                    if current < self.min_monitor:
                        new_lr = old_lr / self.factor
                    else:
                        new_lr = old_lr * self.factor
                    new_lr = min(new_lr, self.max_lr)
                    new_lr = max(new_lr, self.min_lr)
                    K.set_value(self.model.optimizer.lr, new_lr)
                    if self.verbose > 0:
                        print('\nEpoch %05d: DynamicLR changing learning '
                              'rate from %s to %s.' % (epoch + 1, old_lr, new_lr))
                    self._reset()
                    self.cooldown_counter = self.cooldown
                    self.wait = 0

    def in_cooldown(self):
        return self.cooldown_counter > 0

dynamic_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=1e-3,verbose=1)
#dynamic_lr = DynamicLR(monitor='val_dice', factor=0.1, patience=3, min_lr=1e-2, verbose=1)

history=model.fit(train_set, epochs=EPOCH, validation_data=(x_test, y_test), validation_steps=BATCH_SIZE, callbacks=[tb_callback, checkpointer, dynamic_lr])
#history=model.fit(train_set, epochs=EPOCH, callbacks=[tb_callback, checkpointer, dynamic_lr])
#history=model.fit(train_set, epochs=EPOCH, validation_data=(x_test, y_test), validation_steps=BATCH_SIZE, callbacks=[tb_callback, checkpointer])
#history=model.fit(train_set, epochs=EPOCH,  callbacks=[tb_callback])


zero_m=tf.zeros_like(y_test)
print("zero accuracy for cal_accuracy is ", cal_accuracy(y_test, zero_m))
print("self accuracy for cal_accuracy is ", cal_accuracy(y_test, y_test))
print("zero dice for cal_dice is ", cal_dice(y_test, zero_m, is_to_mask=False,loss_type = 'jaccard'))
print("self dice for cal_dice is ", cal_dice(y_test, y_test, is_to_mask=False,loss_type = 'jaccard'))
print("zero focal for cal_focal is ", cal_focal(y_test, zero_m, is_to_mask=False))
print("self focal for cal_focal is ", cal_focal(y_test, y_test, is_to_mask=False))
print("zero focal mask for cal_focal is ", cal_focal(y_test, zero_m, is_to_mask=True))
print("self focal mask for cal_focal is ", cal_focal(y_test, y_test, is_to_mask=True))
print("history is ",history)

#model = keras.models.load_model(CKPT_FILE)
model.load_weights(CKPT_FILE)

pred=model.predict(x_test)
print("accuracy for pred is ", cal_accuracy(y_test, pred))
print("dice for pred is ", cal_dice(y_test, pred, is_to_mask=True,loss_type = 'jaccard'))
print("mask dice for pred is ", cal_dice(y_test, pred, is_to_mask=False,loss_type = 'jaccard'))


# or save to csv:
outputFolder=os.path.dirname(CSV_FILE)
if not os.path.exists(outputFolder):
    os.makedirs(outputFolder)
with open(CSV_FILE, mode='w') as f:
    hist_df = pd.DataFrame(history.history)
    hist_df.to_csv(f, index_label="epoch")
