{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RUP-Net.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPSXYSuOC1S0KmdiT+9MZ3O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyaochn/RUP-Net/blob/master/RUP_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcOmFvrdKL7z",
        "colab_type": "text"
      },
      "source": [
        "# **Residual U-Net + Pixel De-convolution Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvX41AssKCUF",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   Import library\n",
        "2.   Mount Google Drive to load dataset. Att: /X, /Y to load correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJjBxVENhJ50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"tensorflow version: \",tf.__version__)\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "import scipy.ndimage as snd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bExW2jRnhSyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('gdrive')\n",
        "#!ls \"gdrive/My Drive/Deep-Learning/Data\"\n",
        "\n",
        "\n",
        "#!pip install tensorflow_io\n",
        "#\n",
        "#import tensorflow_io as tfio\n",
        "#print(\"tensorflow_io version: \",tfio.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ftD8k7AKG8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#number of training cases used in one step\n",
        "BATCH_SIZE=1\n",
        "#one epoch trains all training cases, One epoch=BATCH_SIZE*steps\n",
        "EPOCH=5\n",
        "\n",
        "#3D image, train set (batch, in_depth, in_height, in_width, in_channels)\n",
        "CHANNEL_AXIS=4\n",
        "\n",
        "#number of filters at start node\n",
        "START_NUM_OF_FILTERS=32\n",
        "\n",
        "#number of class, for classification = 2\n",
        "NUM_CLASS=2\n",
        "\n",
        "#number of epoch to augment, if 2, 4*orig size new images are generated\n",
        "#because one epoch including flip and rotate\n",
        "AUGMENT_EPOCH=0 \n",
        "\n",
        "#Use residual network\n",
        "ISRES=True\n",
        "#Use MaxPooling or not\n",
        "ISPOOL=False\n",
        "#Deconvolution layer: deconv, pdn, upsampling\n",
        "DECONV=\"deconv\"\n",
        "#loss function name: focal, dice, default\n",
        "LOSS=\"default\"\n",
        "\n",
        "#use Add or Concat in up-sampling\n",
        "ISADD=True\n",
        "\n",
        "#U-Net_DEPTH=first layer to bottom layer in U-Net, so there are layer_depth-1 bridges\n",
        "UNET_DEPTH=2\n",
        "\n",
        "\n",
        "\n",
        "root_dir=\"gdrive/My Drive/Deep-Learning/\"\n",
        "train_fn=root_dir+\"Data/train.h5\"\n",
        "test_fn=root_dir+\"Data/test.h5\"\n",
        "\n",
        "#!ls \"$train_fn\"\n",
        "\n",
        "#x_train, y_train= tfio.IODataset.from_hdf5(train_fn, dataset='/X'), tfio.IODataset.from_hdf5(train_fn, dataset='/Y')\n",
        "#x_test, y_test= tfio.IODataset.from_hdf5(test_fn, dataset='/X'), tfio.IODataset.from_hdf5(test_fn, dataset='/Y')\n",
        "\n",
        "#conf=tf.io.read_file(root_dir+\"Configures/train.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB9lfYFXtna-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#last channel axis is also number of class (1+1=2)\n",
        "#image_shape=x_train.element_spec.shape\n",
        "#print(image_shape)\n",
        "\n",
        "with h5py.File(train_fn,'r') as train_f:\n",
        "  x_train, y_train=np.array(train_f['X']), np.array(train_f['Y'])\n",
        "  #x_train=x_train/np.amax(x_train)\n",
        "\n",
        "#expand (B,D,W,H, Channel=1) to (B,D,W,H, START_NUM_OF_FILTERS) so I don't need concat first input layer multiple times.\n",
        "#same as x_test\n",
        "x_train=tf.repeat(x_train, START_NUM_OF_FILTERS, axis=CHANNEL_AXIS)\n",
        "\n",
        "SHUFFLE_SIZE=BATCH_SIZE*np.prod(y_train.shape)\n",
        "\n",
        "y_train=tf.one_hot(y_train, 2, axis=CHANNEL_AXIS, dtype=tf.int32)\n",
        "train_set=tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "\n",
        "\n",
        "with h5py.File(test_fn,'r') as test_f:\n",
        "  x_test, y_test=np.array(test_f['X']), np.array(test_f['Y'])\n",
        "  #x_test=x_test/np.amax(x_test)\n",
        "\n",
        "x_test=tf.repeat(x_test, START_NUM_OF_FILTERS, axis=CHANNEL_AXIS)\n",
        "y_test=tf.one_hot(y_test, 2, axis=CHANNEL_AXIS, dtype=tf.int32)\n",
        "#print(x_train.shape, y_train.shape)\n",
        "#print(x_test.shape, y_test.shape)\n",
        "\n",
        "print(train_set.element_spec)\n",
        "print(np.amax(x_train))\n",
        "print(np.amax(y_train))\n",
        "print(np.amax(x_test))\n",
        "print(np.amax(y_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdyrGns1eNiE",
        "colab_type": "text"
      },
      "source": [
        "Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5qJsrzkdrJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://www.wouterbulten.nl/blog/tech/data-augmentation-using-tensorflow-data-dataset/#rotation-and-flipping\n",
        "def plot_dataset(dataset, n_cases=8, n_slices_per_case=10):\n",
        "  num_of_sets=len(dataset.element_spec)\n",
        "  img_shape=dataset.element_spec[0].shape\n",
        "  #[0]=slice/depth, [1]=width, [2]=height, [3]=color\n",
        "  d,w,h,c=0,1,2,3\n",
        "  dep,wid,hei=img_shape[d], img_shape[w], img_shape[h]\n",
        "  slices=np.rint(np.linspace(0,1,n_slices_per_case)*(dep-1)).astype(np.int32)\n",
        "  output = np.zeros((num_of_sets, hei * n_slices_per_case,wid * n_cases))\n",
        "\n",
        "  i=0\n",
        "  for case in dataset.take(n_cases):\n",
        "    for j in range(num_of_sets):\n",
        "      input=case[j].numpy()\n",
        "      input=input[slices,:,:,0]\n",
        "      output[j, :,i*wid:(i+1)*wid]=np.vstack(input)\n",
        "    i += 1\n",
        "\n",
        "  fig, ax = plt.subplots(1,num_of_sets, figsize=(15,15))\n",
        "  for i in range(num_of_sets):\n",
        "    #plt.imshow(output, extent=[1,n_cases, slices[-1], slices[0]])\n",
        "    img=ax[i].imshow(output[i,:,:], cmap=\"gray\", aspect=\"auto\")\n",
        "    xtick=np.arange(1,n_cases+1)\n",
        "    ytick=slices/(slices[1]-slices[0])\n",
        "    ax[i].set_xticks(xtick*wid-wid*0.5)\n",
        "    ax[i].set_xticklabels(xtick)\n",
        "    ax[i].set_yticks(ytick*hei+hei*0.5)\n",
        "    ax[i].set_yticklabels(slices)\n",
        "    #ax[i].set_title(\"Title\")\n",
        "    ax[i].set_xlabel(\"Case #\")\n",
        "    ax[i].set_ylabel(\"Slice #\")\n",
        "    fig.show(img)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IVAgAANl20O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def aug_rotate(img):\n",
        "  nb=img.shape[0]\n",
        "  outimg=[]\n",
        "  for i in range(nb):\n",
        "    outimg.append(tf.image.rot90(img[i,:,:,:], tf.random.uniform([], minval=1, maxval=4, dtype=tf.int32)))\n",
        "  outimg=tf.stack(outimg)\n",
        "  return outimg\n",
        "\n",
        "def aug_flip(img):\n",
        "  nb=img.shape[0]\n",
        "  outimg=[]\n",
        "  for i in range(nb):\n",
        "    outimg.append(tf.image.flip_left_right(img[i,:,:,:]))\n",
        "  outimg=tf.stack(outimg)\n",
        "  return outimg\n",
        "\n",
        "def dataset_len(dataset):\n",
        "  i=0\n",
        "  for e in dataset:\n",
        "    i +=1\n",
        "  return i\n",
        "#for i in train_set.take(1):\n",
        "#  ximg=i[0][45,:,:,0]\n",
        "#  yimg=i[1][45,:,:,0]\n",
        "\n",
        "\n",
        "if AUGMENT_EPOCH>0:\n",
        "  augmentations = [aug_flip, aug_rotate]\n",
        "\n",
        "  print(\"Before augmentation, train_set size = \",dataset_len(train_set))\n",
        "  orig=train_set\n",
        "  for i in range(AUGMENT_EPOCH):\n",
        "    for f in augmentations:\n",
        "      train_set=train_set.concatenate(orig.map(lambda x,y: (f(x), f(y)), num_parallel_calls=tf.data.experimental.AUTOTUNE))\n",
        "  print(\"After augmentation, train_set size = \",dataset_len(train_set))\n",
        "#for i in xxx.take(1):\n",
        "#  ximg_aug=i[0][45,:,:,0]\n",
        "#  yimg_aug=i[1][45,:,:,0]\n",
        "\n",
        "#fig, ax = plt.subplots(2, 2)\n",
        "#ax[0,0].imshow(ximg,cmap=\"gray\")\n",
        "#ax[0,1].imshow(ximg_aug,cmap=\"gray\")\n",
        "#ax[1,0].imshow(yimg, cmap=\"gray\")\n",
        "#ax[1,1].imshow(yimg_aug, cmap=\"gray\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJDgtvXXeJ1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_set=train_set.batch(BATCH_SIZE).shuffle(SHUFFLE_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "train_set=train_set.batch(BATCH_SIZE).shuffle(SHUFFLE_SIZE)\n",
        "print(train_set.element_spec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyrKfyccyqpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def node(input_layer, filters, conv_size, name=None, isres=True, ispool=False, isadd=True):\n",
        "  with tf.name_scope(name): #it seems doens't work\n",
        "    x=input_layer\n",
        "    num_sublayers=2\n",
        "    for i in range(num_sublayers):\n",
        "      str_i=str(i)\n",
        "      x = keras.layers.Conv3D((num_sublayers-i)*filters, conv_size, activation='relu', padding='same', \n",
        "                              kernel_regularizer=keras.regularizers.l2(0.01), bias_regularizer=keras.regularizers.l2(0.01),\n",
        "                              name=name+\"/Conv3D_\"+str_i)(x)\n",
        "      x = keras.layers.BatchNormalization(name=name+\"/BN_\"+str_i)(x)\n",
        "      x = keras.layers.Dropout(0.5, name=name+\"/Dropout_\"+str_i)(x)\n",
        "    if isres:\n",
        "      addname=\"/Add_0\"\n",
        "      if isadd:\n",
        "        x = keras.layers.Add(name=name+addname)([x, input_layer])\n",
        "      else:\n",
        "        addname=\"/Concat_0\"\n",
        "        x = keras.layers.Concatenate(name=name+addname)([x, input_layer])\n",
        "      x = keras.layers.BatchNormalization(name=name+addname+\"/BN_0\")(x)\n",
        "    if ispool:\n",
        "      x = keras.layers.MaxPooling3D(padding=\"same\", name=name+\"/Pool_0\")(x)\n",
        "    x = keras.layers.Activation('relu',name=name+\"/Activation_0\")(x)\n",
        "    return x\n",
        "\n",
        "def u_net(input_layer, filters, conv_size, name=None, isres=True, ispool=False, \n",
        "          layer_depth=4, deconv=\"deconv\", isadd=True):\n",
        "  with tf.name_scope(name):\n",
        "    x=input_layer\n",
        "    downlist=[] #including input/output of each node in down-sampling operation\n",
        "\n",
        "    #down-sampling\n",
        "    for i in range(layer_depth-1):\n",
        "      downlist.append(x)\n",
        "      node_name=\"/Down_\"+str(i)\n",
        "      x=node(x, filters, conv_size, name+node_name, isres, ispool, isadd)\n",
        "      \n",
        "    downlist.append(x)\n",
        "    \n",
        "    #bottom\n",
        "    node_name=\"/Bottom\"\n",
        "    x=node(x, filters, conv_size, name+node_name, isres, ispool, isadd)\n",
        "\n",
        "\n",
        "    def decoder(x, filters, conv_size, strides, name, deconv=\"deconv\"):\n",
        "      decname=\"/DeConv_Conv3DTrans_0\"\n",
        "      if deconv==\"deconv\":\n",
        "        x=keras.layers.Conv3DTranspose(filters, conv_size, strides=strides, padding=\"same\", \n",
        "                                       kernel_regularizer=keras.regularizers.l2(0.01),\n",
        "                                       bias_regularizer=keras.regularizers.l2(0.01), \n",
        "                                       name=name+decname)(x)\n",
        "      elif deconv==\"upsampling\":\n",
        "        decname=\"/DeConv_UpSampling_0\"\n",
        "        #this 2 is same as one used for MaxPooling, so if pool changes, this will change\n",
        "        x=keras.layers.UpSampling3D(strides, name=name+decname)(x)\n",
        "      else:\n",
        "        #implenment PDN here\n",
        "        x=x        \n",
        "      x=keras.layers.BatchNormalization(name=name+decname+\"/BN_0\")(x)\n",
        "      return x\n",
        "\n",
        "    #up-sampling\n",
        "    strides=int(ispool)+1\n",
        "    for i in range(layer_depth-2, -1, -1): #backward like 2,1,0 if dep=4\n",
        "      node_name=\"/Up_\"+str(i)\n",
        "      x=decoder(x, filters, conv_size, strides, name+node_name, deconv)\n",
        "      if isadd:\n",
        "        x=keras.layers.Add(name=name+node_name+\"/Add_Down_\"+str(i))([x, downlist[i+1]])\n",
        "      else:\n",
        "        x=keras.layers.Concatenate(name=name+node_name+\"/Concat_Down_\"+str(i))([x, downlist[i+1]])\n",
        "      #no pooling in up-sampling\n",
        "      x=node(x, filters, conv_size, name+node_name, isres, ispool=False, isadd=isadd)\n",
        "    \n",
        "    x=decoder(x, filters, conv_size, strides, name+node_name+\"/Output\", deconv)\n",
        "    if isadd:\n",
        "      x=keras.layers.Add(name=name+\"/Add_Input\")([x, downlist[0]])\n",
        "    else:\n",
        "      x=keras.layers.Concatenate(name=name+\"/Concat_Input\")([x, downlist[0]])\n",
        "    \n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FICHXIxy1on4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.name_scope(\"Input\"):\n",
        "  input_layer = keras.Input(shape=x_train.shape[1:], name=\"Input\")\n",
        "  x=input_layer\n",
        "  ##use Conv3D to make (?, 64,64,64,1) to (?,64,64,64,32)\n",
        "  ##x = keras.layers.Conv3D(START_NUM_OF_FILTERS, kernel_size=3, padding=\"same\",activation='relu', name=\"Input/Conv3D\")(x)\n",
        "  ##use Concat to make (?, 64,64,64,1) to (?,64,64,64,32)\n",
        "  #for i in range(START_NUM_OF_FILTERS-1):\n",
        "  #  x=keras.layers.Concatenate(axis=CHANNEL_AXIS)([x, input_layer])\n",
        "  x = keras.layers.BatchNormalization(name=\"Input/BN\")(x)\n",
        "x = u_net(input_layer=x, filters=START_NUM_OF_FILTERS, conv_size=3, name = \"U-Net\", isres=ISRES, ispool=ISPOOL, \n",
        "          layer_depth=UNET_DEPTH, deconv=DECONV, isadd=False)\n",
        "\n",
        "x = keras.layers.Conv3D(NUM_CLASS, kernel_size=3, padding=\"same\",activation='relu', name=\"Output\")(x)\n",
        "\n",
        "model = keras.Model(input_layer, x, name=\"RUP-Net\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFnA43DW_3B5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FocalLoss(keras.losses.Loss):\n",
        "    def __init__(self, alpha=0.25, gamma=2,\n",
        "                 reduction=keras.losses.Reduction.AUTO,\n",
        "                 name='focal_loss'):\n",
        "        super().__init__(reduction=reduction, name=name)\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "      with tf.name_scope(\"focal_loss\"):\n",
        "        y_true=tf.cast(y_true, tf.float32)\n",
        "        sigmoid_p = tf.nn.sigmoid(y_pred)\n",
        "        zeros = tf.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
        "\n",
        "        # For poitive prediction, only need consider front part loss, back part is 0;\n",
        "        # y_true > zeros <=> z=1, so poitive coefficient = z - p.\n",
        "        pos_p_sub = tf.where(y_true > zeros, y_true - sigmoid_p, zeros)\n",
        "\n",
        "        # For negative prediction, only need consider back part loss, front part is 0;\n",
        "        # y_true > zeros <=> z=1, so negative coefficient = 0.\n",
        "        neg_p_sub = tf.where(y_true > zeros, zeros, sigmoid_p)\n",
        "        per_entry_cross_ent = - self.alpha * (pos_p_sub ** self.gamma) * tf.math.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
        "                              - (1 - self.alpha) * (neg_p_sub ** self.gamma) * tf.math.log(\n",
        "            tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
        "        return tf.reduce_sum(per_entry_cross_ent) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdSgxMy7GI-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_dice(y_true, y_pred, loss_type, smooth):\n",
        "  y_true_f = tf.reshape(y_true, [-1])\n",
        "  y_true_f = tf.cast(y_true_f, tf.float32)\n",
        "  y_pred_f = tf.reshape(y_pred, [-1])\n",
        "  intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "  if loss_type == 'jaccard':\n",
        "      union = tf.reduce_sum(tf.square(y_pred_f)) + tf.reduce_sum(tf.square(y_true_f))\n",
        "  elif loss_type == 'sorensen':\n",
        "      union = tf.reduce_sum(y_pred_f) + tf.reduce_sum(y_true_f)\n",
        "  else:\n",
        "      raise ValueError(\"Unknown `loss_type`: %s\" % loss_type) \n",
        "  res = (2 * intersection + smooth) / (union + smooth)\n",
        "  return res\n",
        "\n",
        "class Dice(keras.metrics.Metric):\n",
        "    def __init__(self, loss_type='jaccard', smooth=0, name='dice', **kwargs):\n",
        "      super(Dice, self).__init__(name=name, **kwargs)\n",
        "      self.dice = self.add_weight(name='dice', initializer='zeros')\n",
        "      self.loss_type=loss_type\n",
        "      self.smooth=smooth\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "      self.dice.assign(cal_dice(y_true, y_pred, self.loss_type, self.smooth))\n",
        "\n",
        "    def result(self):\n",
        "      return self.dice\n",
        "\n",
        "    def reset_states(self):\n",
        "      # The state of the metric will be reset at the start of each epoch.\n",
        "      self.dice.assign(0.)\n",
        "\n",
        "class DiceLoss(keras.losses.Loss):\n",
        "    def __init__(self, loss_type='jaccard', smooth=0,\n",
        "                 reduction=keras.losses.Reduction.AUTO,\n",
        "                 name='dice_loss'):\n",
        "        super().__init__(reduction=reduction, name=name)\n",
        "        self.loss_type=loss_type\n",
        "        self.smooth=smooth\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "      with tf.name_scope(\"dice_loss\"):\n",
        "        #dice=Dice(self.loss_type, self.smooth, name='diceloss')\n",
        "        dice=cal_dice(y_true, y_pred, self.loss_type, self.smooth)\n",
        "        return (1-dice)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Enfe7UfCAVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if LOSS==\"focal\":\n",
        "  loss=FocalLoss(alpha=0.25, gamma=2),\n",
        "elif LOSS==\"dice\":\n",
        "  loss=DiceLoss(loss_type='jaccard', smooth=1, name='dice')\n",
        "else:\n",
        "  loss=keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(),\n",
        "              loss=loss,\n",
        "              metrics=['accuracy', Dice(loss_type='jaccard', smooth=1, name='dice')])\n",
        "#model.summary()\n",
        "#keras.utils.plot_model(model, 'RUP-Net.png', show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXfstnXI0uqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "#model.fit(train_set, epochs=EPOCH, validation_data=(x_test, y_test), validation_steps=BATCH_SIZE, callbacks=[earlystop])\n",
        "model.fit(train_set, epochs=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}